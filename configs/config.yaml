# MIRADOR-CORE Configuration File
# Complete configuration with all available options

environment: development
port: 8010
log_level: info

# VictoriaMetrics Ecosystem Configuration
database:
  victoria_metrics:
    endpoints:
      - "http://vm-select-0.vm-select:8428"
      - "http://vm-select-1.vm-select:8428"
    timeout: 30000
    username: ""
    password: "" # Set via environment variable
    cluster_mode: true  # Use /select/0/prometheus paths for cluster deployments
    # Optional: enable DNS-based discovery of vmselect pods behind a headless Service
    # discovery:
    #   enabled: true
    #   service: "vm-select.vm-select.svc.cluster.local"
    #   port: 8481
    #   scheme: "http"
    #   refresh_seconds: 30
    #   use_srv: false
    
  victoria_logs:
    endpoints:
      - "http://vl-select-0.vl-select:9428"
      - "http://vl-select-1.vl-select:9428"
    timeout: 30000
    username: ""
    password: ""
    # discovery:
    #   enabled: true
    #   service: "vl-select.vl-select.svc.cluster.local"
    #   port: 9428
    #   scheme: "http"
    #   refresh_seconds: 30
    #   use_srv: false
    
  victoria_traces:
    endpoints:
      - "http://vt-select-0.vt-select:10428"
      - "http://vt-select-1.vt-select:10428"
    timeout: 30000
    username: ""
    password: ""
    # discovery:
    #   enabled: true
    #   service: "vt-select.vt-select.svc.cluster.local"
    #   port: 10428
    #   scheme: "http"
    #   refresh_seconds: 30
    #   use_srv: false

# AI Engines gRPC Configuration
grpc:
  predict_engine:
    endpoint: "predict-engine.mirador:9091"
    models:
      - "isolation_forest"
      - "lstm_trend" 
      - "anomaly_detector"
      - "ensemble_predictor"
    timeout: 30000
    
  rca_engine:
    endpoint: "rca-engine.mirador:9092"
    correlation_threshold: 0.85
    timeout: 30000
    
  alert_engine:
    endpoint: "alert-engine.mirador:9093"
    rules_path: "/etc/mirador/alert-rules.yaml"
    timeout: 30000

# Authentication and Authorization (handled externally - this section is deprecated)
# Mirador-core is designed to run behind an external API gateway or service mesh
# that handles authentication and RBAC.
auth: {}

# Valkey Cluster Caching
cache:
  nodes:
    - "localhost:6379"
  ttl: 300 # 5 minutes default
  password: "" # Set via environment variable
  db: 0

# Cross-Origin Resource Sharing
cors:
  allowed_origins:
    - "https://mirador.company.com"
    - "https://mirador-ui.company.com"
  allowed_methods:
    - "GET"
    - "POST"
    - "PUT"
    - "DELETE"
    - "OPTIONS"
  allowed_headers:
    - "Content-Type"
    - "Authorization"
  exposed_headers:
    - "X-Cache"
    - "X-Rate-Limit-Remaining"
    - "X-Rate-Limit-Reset"
  allow_credentials: true
  max_age: 3600

# External Integrations
integrations:
  slack:
    webhook_url: "" # Set via environment variable
    channel: "#mirador-alerts"
    enabled: false
    
  ms_teams:
    webhook_url: "" # Set via environment variable
    enabled: false
    
  email:
    smtp_host: "smtp.company.com"
    smtp_port: 587
    username: "mirador@company.com"
    password: "" # Set via environment variable
    from_address: "mirador@company.com"
    enabled: false

# Real-time WebSocket Streaming
websocket:
  enabled: true
  max_connections: 1000
  read_buffer_size: 1024
  write_buffer_size: 1024
  ping_interval: 30 # seconds
  max_message_size: 1048576 # 1MB

# Self-monitoring Configuration
monitoring:
  enabled: true
  metrics_path: "/metrics"
  prometheus_enabled: true
  tracing_enabled: false

# Schema Store Configuration (Weaviate)
weaviate:
  enabled: true
  scheme: "http"
  host: "localhost"
  port: 8080
  api_key: "" # Set via environment variable
  consistency: "quorum"
  use_official: true
  nested_keys:
    - "tags"
    - "examples"
    - "allowed_values"

# Search Engine Configuration
search:
  default_engine: "lucene"
  enable_bleve: true
  enable_lucene: true

# Unified Query Engine Configuration (Phase 1.5)
unified_query:
  enabled: true
  cache_ttl: 5m
  max_cache_ttl: 1h
  default_limit: 1000
  enable_correlation: true

uploads:
  # Bulk CSV upload limit (bytes); default 5 MiB.
  bulk_max_bytes: 5242880
  jaeger_endpoint: "http://jaeger:14268/api/traces"

# Engine configuration (Correlation & RCA) - sample values
engine:
  min_window: 10s
  max_window: 1h
  default_graph_hops: 2
  default_max_whys: 5
  ring_strategy: auto
  buckets:
    core_window_size: 30s
    pre_rings: 2
    post_rings: 1
    ring_step: 15s
  min_correlation: 0.6
  min_anomaly_score: 0.7
  strict_time_window: false
  # Default list of metric probes used to seed impact/candidate KPI discovery.
  probes:
    - "db_ops_total"
    - "kafka_consume_total"
    - "kafka_produce_total"
    - "http_request_duration_seconds_count"
    - "db_latency_seconds_count"
    - "transactions_failed_total"
    - "http_errors_total"
  # Services used as candidates for traces fallback/search heuristics.
  service_candidates:
    - "api-gateway"
    - "tps"
    - "keydb-client"
    - "kafka-producer"
    - "kafka-consumer"
    - "cassandra-client"
  # Default limits for queries
  default_query_limit: 1000
  labels:
    service:
      - "service"
      - "service.name"
      - "serviceName"
    pod:
      - "pod"
      - "kubernetes.pod_name"
    namespace:
      - "namespace"
      - "kubernetes.namespace_name"
    deployment:
      - "deployment"
    container:
      - "container"
      - "kubernetes.container_name"
    host:
      - "host"
      - "hostname"
    level:
      - "level"
      - "severity"

  # Platform-standard OTel spanmetrics/servicegraph/isolationforest definitions.
  # These are canonical telemetry connector and processor definitions used
  # by the Correlation and RCA engines. Do not hardcode raw metric or label
  # names in code; they must be read from this config or seeded into the
  # KPI registry (Stage-00 / Weaviate) via the bootstrap step.
  telemetry:
    connectors:
      spanmetrics:
        kind: connector
        metrics:
          - name: traces_span_metrics_calls
            type: counter
            description: "Count of spans (requests)"
            labels:
              - service_name
              - span_name
              - span_kind
              - status_code

          - name: traces_span_metrics_duration
            type: histogram
            description: "Span duration histogram"
            labels:
              - service_name
              - span_name
              - span_kind
              - status_code
              - le

      servicegraph:
        kind: connector
        metrics:
          - name: traces_service_graph_request_total
            type: counter
            description: "Total count of requests between client and server nodes"
            labels:
              - client
              - server
              - connection_type

          - name: traces_service_graph_request_failed_total
            type: counter
            description: "Total count of failed requests between client and server nodes"
            labels:
              - client
              - server
              - connection_type

          - name: traces_service_graph_request_server
            type: histogram
            description: "Request duration seconds measured from the server side"
            labels:
              - client
              - server
              - connection_type
              - le

          - name: traces_service_graph_request_client
            type: histogram
            description: "Request duration seconds measured from the client side"
            labels:
              - client
              - server
              - connection_type
              - le

          - name: traces_service_graph_unpaired_spans_total
            type: counter
            description: "Total count of unpaired spans"
            labels:
              - client
              - server
              - connection_type

          - name: traces_service_graph_dropped_spans_total
            type: counter
            description: "Total count of dropped spans"
            labels:
              - client
              - server
              - connection_type

    processors:
      isolationforest:
        kind: processor
        labels:
          - iforest_is_anomaly
          - iforest_anomaly_score
          - service_name
          - span_name
          - span_kind
          - status_code
          - le

# MIRA (Mirador Intelligent Research Assistant) - AI-powered RCA Explanations
# MIRA-001: Feature for generating non-technical explanations from RCA data
mira:
  enabled: false  # Disabled by default (enable in dev/prod as needed)
  provider: "ollama"  # Options: openai, anthropic, vllm, ollama
  timeout: 5m  # Increased to 5 minutes for large RCA analysis with multiple chains
  
  # Cache strategy for AI responses
  cache_strategy:
    enabled: true
    use_valkey_for_fast_cache: true
    ttl: 3600  # 1 hour cache for explanations
  
  # Rate limiting for MIRA endpoints
  rate_limit:
    enabled: true
    requests_per_minute: 10
    burst: 5
  
  # OpenAI configuration (when provider=openai)
  openai:
    api_key: "${OPENAI_API_KEY}"  # Set via environment variable
    model: "gpt-4"  # Or: gpt-3.5-turbo
    max_tokens: 2000
  
  # Anthropic configuration (when provider=anthropic)
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"  # Set via environment variable
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 2000
  
  # vLLM configuration (when provider=vllm)
  vllm:
    endpoint: "http://vllm-server:8000/v1/completions"
    model: "meta-llama/Llama-3.1-70B-Instruct"
    max_tokens: 2000
  
  # Ollama configuration (when provider=ollama) - Local, free option
  ollama:
    endpoint: "http://localhost:11434/api/generate"
    model: "llama3.2:3b"  # 3B model (2GB, 128K context) - optimized for M1 Apple Silicon
    max_tokens: 2000
  
  # Prompt template for generating explanations
  prompt_template: |
    You are MIRA (Mirador Intelligent Research Assistant), an AI assistant that explains complex technical root cause analysis to non-technical stakeholders.

    METHODOLOGY CONTEXT:
    This RCA analysis follows the '5 Whys' methodology - a systematic approach to finding root causes by asking "Why?" multiple times:
    - Why #1: What did users/business experience? (IMPACT layer - the visible symptom)
    - Why #2-4: How did the issue propagate through the system? (Intermediate steps)
    - Why #5: What is the fundamental root cause? (CAUSE layer - the underlying technical issue)

    KPI LAYER CLASSIFICATION:
    - IMPACT Layer KPIs: Business metrics, user experience, customer-facing symptoms (e.g., failed transactions, slow response times)
    - CAUSE Layer KPIs: Technical issues, infrastructure problems, code defects (e.g., database errors, CPU spikes, memory leaks)

    Your task is to translate the technical '5 Whys' chain into a narrative that business stakeholders can understand.

    Below is RCA data in TOON (Token Oriented Object Notation) format, which describes an incident:

    {{.TOONData}}

    Key Details:
    - Impact Service: {{.ImpactService}}
    - Impact Metric (IMPACT Layer - What users saw): {{.MetricName}}
    - Severity: {{.Severity}}
    - Anomaly Score: {{.AnomalyScore}}
    {{- if .RootCauseService}}
    - Root Cause Service: {{.RootCauseService}}
    - Root Cause Component: {{.RootCauseComponent}}
    {{- end}}
    {{- if .TopChainPath}}
    - Causal Chain Path (The '5 Whys' progression): {{.TopChainPath}}
    {{- end}}

    Please provide a clear, non-technical explanation that:
    1. What happened (IMPACT layer - the user-facing symptom, Why #1)
    2. How it propagated (the intermediate 'Whys' showing the chain of causation)
    3. What caused it (CAUSE layer - the fundamental root cause, Why #5)
    4. Which services/components were affected along the '5 Whys' chain
    5. Why this matters to the business (impact on users, revenue, operations)

    IMPORTANT: Use progression language like "First, users experienced...", "This was caused by...", "Which in turn was triggered by...", "Ultimately, the root cause was...". Think of it as telling a story from symptom to root cause.

    Use simple language suitable for executives or non-technical team members. Avoid jargon, acronyms, and technical metrics unless absolutely necessary.
