# Development Environment Configuration
environment: development
port: 8010
log_level: debug

database:
  victoriametrics:
    endpoints:
      - "http://victoriametrics:8428"
    timeout: 10000
    cluster_mode: false  # Single-node deployment in localdev
  victorialogs:
    endpoints:
      - "http://victorialogs:9428"
    timeout: 10000
  victoriatraces:
    endpoints:
      - "http://victoriatraces:10428"
    timeout: 10000

grpc:
  predict_engine:
    endpoint: "127.0.0.1:9091"
    models: ["isolation_forest", "lstm_trend"]
    timeout: 10000
  rca_engine:
    endpoint: "127.0.0.1:8020"
    correlation_threshold: 0.8
    timeout: 10000
  alert_engine:
    endpoint: "127.0.0.1:9093"
    timeout: 10000

auth:
  ldap:
    enabled: false
  oauth:
    enabled: false
  # RBAC is now handled externally

cache:
  nodes:
    - "localhost:6379"
  ttl: 60 # Shorter TTL for development

cors:
  allowed_origins:
    - "http://localhost:3000"
    - "http://localhost:3001"
    - "http://127.0.0.1:3000"

integrations:
  slack:
    enabled: false
  ms_teams:
    enabled: false
  email:
    enabled: false

websocket:
  enabled: true
  max_connections: 100
  ping_interval: 10

monitoring:
  enabled: true
  prometheus_enabled: true
  tracing_enabled: true

# Search Engine Configuration
search:
  default_engine: "lucene"
  enable_bleve: true
  enable_lucene: true
  query_cache:
    enabled: true
    ttl: 1800
  bleve:
    logs_enabled: true
    traces_enabled: true
    metrics_enabled: true
    index_path: "/var/lib/mirador/bleve"
    batch_size: 1000
    max_memory_mb: 256  # Lower for development
    memory_optimization:
      object_pooling: true
      adaptive_cache: true
    storage:
      memory_cache_ratio: 0.5
      disk_cache_ratio: 0.5
      max_concurrent_queries: 10  # Lower for development
    metrics_sync:
      enabled: true
      strategy: "incremental"
      interval: 5m  # More frequent for development
      full_sync_interval: 1h
      batch_size: 500  # Smaller batches for development
      max_retries: 3
      retry_delay: 10s
      time_range_lookback: 30m  # Shorter lookback for development
      shard_count: 2  # Fewer shards for development

# Unified Query Engine Configuration (Phase 1.5)
unified_query:
  enabled: true
  cache_ttl: 1m  # Shorter for development
  max_cache_ttl: 10m
  default_limit: 100
  enable_correlation: true  # Enable correlation in development

# Stage-01 Correlation & RCA Engine Configuration (AGENTS.md ยง3.1)
engine:
  strict_timewindow_payload: true  # Enforce time-window-only API contract

uploads:
  # Configurable bulk CSV upload limit (bytes). Default is 5 MiB if omitted.
  bulk_max_bytes: 5242880

# Schema Store Configuration (Weaviate)
weaviate:
  enabled: true
  scheme: "http"
  host: "localhost"
  port: 8080
  api_key: "" # Set via environment variable
  consistency: "quorum"
  use_official: true
  nested_keys:
    - "tags"
    - "examples"
    - "allowed_values"

# MIRA (Mirador Intelligent Research Assistant) - AI-powered RCA Explanations
# MIRA-001: Enabled in development for testing
mira:
  enabled: true  # Enabled for local development
  provider: "ollama"  # Using free local Ollama for dev
  timeout: 300s  # Increased for 3B model on CPU (first request can take 2-3 minutes for chunked analysis)
  
  cache_strategy:
    enabled: true
    use_valkey_for_fast_cache: true
    ttl: 1800  # 30 minutes cache in dev (shorter for testing)
  
  rate_limit:
    enabled: false  # Disabled for development ease
    requests_per_minute: 100
    burst: 20
  
  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-3.5-turbo"  # Cheaper model for dev
    max_tokens: 1500
  
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    model: "claude-3-5-sonnet-20241022"
    max_tokens: 1500
  
  vllm:
    endpoint: "http://localhost:8000/v1/completions"
    model: "meta-llama/Llama-3.1-70B-Instruct"
    max_tokens: 1500
  
  ollama:
    endpoint: "${OLLAMA_ENDPOINT}"
    model: "llama3.2:3b"  # 3B model for better quality on M1 (2GB, 128K context, optimized for Apple Silicon)
    max_tokens: 1500
  
  prompt_template: |
    You are MIRA (Mirador Intelligent Research Assistant), an AI assistant that explains complex technical root cause analysis to non-technical stakeholders.

    Below is RCA data in TOON (Token Oriented Object Notation) format, which describes an incident:

    {{.TOONData}}

    Key Details:
    - Impact Service: {{.ImpactService}}
    - Metric: {{.MetricName}}
    - Severity: {{.Severity}}
    - Anomaly Score: {{.AnomalyScore}}
    {{- if .RootCauseService}}
    - Root Cause Service: {{.RootCauseService}}
    - Root Cause Component: {{.RootCauseComponent}}
    {{- end}}
    {{- if .TopChainPath}}
    - Causal Chain: {{.TopChainPath}}
    {{- end}}

    Please provide a clear, non-technical explanation of:
    1. What happened (in simple terms)
    2. What caused it (the root cause)
    3. Which services/components were affected
    4. Why this matters to the business

    Use simple language suitable for executives or non-technical team members. Avoid jargon, acronyms, and technical metrics unless absolutely necessary.
