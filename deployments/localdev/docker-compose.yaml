services:
  # ──────────────────────────────────────────────
  # VictoriaMetrics Suite
  # ──────────────────────────────────────────────
  victoriametrics:
    image: victoriametrics/victoria-metrics:latest
    pull_policy: if_not_present
    container_name: victoriametrics
    ports:
      - "8428:8428"   # Expose for external access
    volumes:
      - vmdata:/victoria-metrics-data
    command:
      - --storageDataPath=/victoria-metrics-data
    networks:
      - mirador-net

  victorialogs:
    image: victoriametrics/victoria-logs:latest
    pull_policy: if_not_present
    container_name: victorialogs
    ports:
      - "9428:9428"
    volumes:
      - vldata:/victoria-logs-data
    command:
      - --storageDataPath=/victoria-logs-data
    networks:
      - mirador-net

  victoriatraces:
    image: victoriametrics/victoria-traces:latest
    pull_policy: if_not_present
    container_name: victoriatraces
    ports:
      - "10428:10428"
    volumes:
      - vtdata:/victoria-traces-data
    command:
      - --storageDataPath=/victoria-traces-data
    networks:
      - mirador-net

  # ──────────────────────────────────────────────
  # OpenTelemetry Collector
  # ──────────────────────────────────────────────
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.137.0
    pull_policy: if_not_present
    container_name: otel-collector
    command: ["--config", "/etc/otelcol/config.yaml"]
    depends_on:
      - victoriametrics
      - victorialogs
      - victoriatraces
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
    volumes:
      - ./otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    networks:
      - mirador-net
    healthcheck:
      test: ["CMD", "/otelcol-contrib", "--version"]
      interval: 10s
      timeout: 5s
      retries: 10

  # ──────────────────────────────────────────────
  # Valkey (single-node Redis alternative)
  # ──────────────────────────────────────────────
  valkey:
    image: valkey/valkey:latest
    pull_policy: if_not_present
    container_name: mirador-valkey
    ports:
      - "6379:6379"
    volumes:
      - valkeydata:/data
    networks:
      - mirador-net
    healthcheck:
      test: ["CMD-SHELL", "valkey-cli -h 127.0.0.1 ping | grep -q PONG"]
      interval: 5s
      timeout: 3s
      retries: 10

  # ──────────────────────────────────────────────
  # Weaviate (local vector DB)
  # ──────────────────────────────────────────────
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.8
    pull_policy: if_not_present
    container_name: weaviate
    ports:
      - "8080:8080"
    restart: unless-stopped
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      # Use Weaviate's built-in text2vec-transformers vectorizer for automatic
      # embedding generation. NOTE: the Weaviate image must include this module
      # (official images >= 1.x typically do) *and* you need a transformer
      # inference backend available to serve the model if not using the
      # built-in lightweight inference. For localdev, you can keep this as
      # "none" if you don't want embeddings auto-generated.
      DEFAULT_VECTORIZER_MODULE: "text2vec-transformers"
      # Enable the transformers module in Weaviate runtime. This must match
      # the actual available modules in the image (text2vec-transformers) and
      # points to the inference API service below.
      ENABLE_MODULES: "text2vec-transformers"
      TRANSFORMERS_INFERENCE_API: "http://text2vec-transformers:8080"
      TRANSFORMERS_INFERENCE_MODEL: "sentence-transformers/all-MiniLM-L6-v2"
      # Transformer inference service (optional). If you run a local model
      # server (e.g., huggingface/transformers-inference or text2vec compatible
      # API), point Weaviate at it here. Keep commented or set to a local URL
      # when available.
      # TRANSFORMERS_INFERENCE_API: "http://transformers:8080"
      # TRANSFORMERS_INFERENCE_MODEL: "sentence-transformers/all-MiniLM-L6-v2"
      QUERY_DEFAULTS_LIMIT: "25"
      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
      CLUSTER_GOSSIP_BIND_PORT: "7100"
      CLUSTER_DATA_BIND_PORT: "7101"
      AUTOSCHEMA_ENABLED: "false"
      LOG_LEVEL: "debug"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://127.0.0.1:8080/v1/.well-known/ready"]
      interval: 5s
      timeout: 3s
      retries: 30
    volumes:
      - weavdata:/var/lib/weaviate
    networks:
      - mirador-net

  # ──────────────────────────────────────────────
  # Ollama (Local AI Model Server for MIRA)
  # ──────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    pull_policy: if_not_present
    container_name: ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - mirador-net
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Optimized for Docker CPU: Using llama3.2:1b (740MB, fast inference without GPU)
    # Auto-pull model on first start:
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 5
        ollama pull llama3.2:3b
        wait

  # Optional: local transformer inference server for Weaviate's text2vec-transformers.
  # This provides an ONNX-optimized sentence-transformers model suitable for
  # CPU-based local development. The image is optimized for AMD64 (AVX2) and
  # ARM64 and uses ONNX runtime for efficient inference.
  text2vec-transformers:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2-onnx
    pull_policy: if_not_present
    container_name: text2vec-transformers
    ports:
      - "8081:8080" # expose host:container for testing (internal weaviate calls use the container name)
    restart: unless-stopped
    environment:
      # Model served by this backend. Keep in sync with TRANSFORMERS_INFERENCE_MODEL
      - MODEL_ID=sentence-transformers/all-MiniLM-L6-v2
      - LOG_LEVEL=info
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/health || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    networks:
      - mirador-net

  # ──────────────────────────────────────────────
  # MIRADOR CORE (Main Application)
  # ──────────────────────────────────────────────
  mirador-core:
    build:
      context: ../..
      dockerfile: Dockerfile
    # If you prefer to pull a published image, comment the above build block and use:
    # image: platformbuilds/mirador-core:v2.1.3
    container_name: mirador-core
    restart: unless-stopped
    depends_on:
      valkey:
        condition: service_healthy
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
      victoriametrics:
        condition: service_started
      victorialogs:
        condition: service_started
      victoriatraces:
        condition: service_started
    ports:
      - "8010:8010"
    environment:
      # App basics
      - PORT=8010
      - LOG_LEVEL=debug
      - ENVIRONMENT=development
      - CONFIG_PATH=/configs/config.development.yaml
      - AUTH_ENABLED=true

      # Upload limits
      - BULK_UPLOAD_MAX_BYTES=5242880

      # Weaviate (DEV)
      - WEAVIATE_HOST=weaviate
      - WEAVIATE_PORT=8080
      - WEAVIATE_SCHEME=http
      - WEAVIATE_ENABLED=true
      - WEAVIATE_USE_OFFICIAL=false

      # VictoriaMetrics endpoints
      - VM_ENDPOINTS=http://victoriametrics:8428
      - VL_ENDPOINTS=http://victorialogs:9428
      - VT_ENDPOINTS=http://victoriatraces:10428

      # Valkey (Redis-compatible)
      - VALKEY_CACHE_NODES=valkey:6379
      - CACHE_TTL=300

      # Stage-01 RCA/Correlation API enforcement (AGENTS.md §3.1)
      - MIRADOR_ENGINE_STRICT_TIMEWINDOW_PAYLOAD=true

      # MIRA (AI-powered RCA explanations)
      - MIRA_ENABLED=true
      - MIRA_PROVIDER=ollama
      - OLLAMA_ENDPOINT=http://ollama:11434/api/generate
    healthcheck:
      test: ["CMD", "/mirador-core", "healthcheck"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - ../../configs:/configs:ro
      - bleve-data:/var/lib/mirador/bleve
    networks:
      - mirador-net

# ──────────────────────────────────────────────
# VOLUMES
# ──────────────────────────────────────────────
volumes:
  vmdata: {}
  vldata: {}
  vtdata: {}
  weavdata: {}
  valkeydata: {}
  bleve-data: {}
  ollama-data: {}

# ──────────────────────────────────────────────
# NETWORKS
# ──────────────────────────────────────────────
networks:
  mirador-net:
    driver: bridge